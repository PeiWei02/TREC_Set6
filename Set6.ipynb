{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to search for files that match a specified pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2023.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to clean the data in a TREC input file\n",
    "def clean_trec_input_file(filename):\n",
    "    # Load the data into a pandas dataframe\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None, names=['query_id', 'ignore', 'doc_id', 'rank', 'score', 'ignore2'])\n",
    "\n",
    "    # Drop any columns that aren't needed\n",
    "    df.drop(['ignore', 'ignore2'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert the rank and score columns to numeric data types\n",
    "    df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "    df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "\n",
    "    # Drop any rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to concatenate the cleaned data from multiple TREC input files\n",
    "def concat_clean_trec_input_files(filenames):\n",
    "    # Initialize an empty list to store the cleaned dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each input file\n",
    "    for filename in filenames:\n",
    "        # Clean the data in the file\n",
    "        df = clean_trec_input_file(filename)\n",
    "\n",
    "        # Append the cleaned dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate the cleaned dataframes\n",
    "    concatenated_df = pd.concat(dfs)\n",
    "\n",
    "    # Reset the index\n",
    "    concatenated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Return the concatenated dataframe\n",
    "    return concatenated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a list of all CSV files in the input directory\n",
    "input_files = glob.glob('input/*.csv')\n",
    "\n",
    "print(input_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate and clean the data in the input files\n",
    "cleaned_data = concat_clean_trec_input_files(input_files)\n",
    "\n",
    "cleaned_data.head()\n",
    "print(cleaned_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the qrels file\n",
    "qrels_file_path = 'qrels.trec8.adhoc.csv'\n",
    "qrels_df = pd.read_csv(qrels_file_path, sep=' ', header=None, names=['query_id', 'ignore', 'doc_id', 'relevance'])\n",
    "\n",
    "#show qrels_dataframe\n",
    "qrels_df.head()\n",
    "print(qrels_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xiao Hei Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['query_id', 'ignore', 'doc_id', 'relevance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the qrels file\n",
    "qrels_file_path = 'qrels.trec8.adhoc.csv'\n",
    "qrels_df = pd.read_csv(qrels_file_path, sep=' ', header=None, names=['query_id', 'ignore', 'doc_id', 'relevance'])\n",
    "\n",
    "#show qrels_dataframe\n",
    "qrels_df.head()\n",
    "print(qrels_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_data: (689266, 6)\n",
      "Shape of cleaned_data: (290603, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files in the input directory\n",
    "input_files = glob.glob('input/*.csv')\n",
    "\n",
    "# Create an empty list to store the input files dataframes\n",
    "input_dfs = []\n",
    "\n",
    "# Loop through each input CSV file\n",
    "for filename in input_files:\n",
    "    # Read the input CSV file into a dataframe\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None)\n",
    "    # Append the dataframe to the list of dataframes\n",
    "    input_dfs.append(df)\n",
    "    \n",
    "raw_data = pd.concat(input_dfs, ignore_index=True)\n",
    "print('Shape of raw_data:', raw_data.shape)\n",
    "\n",
    "############################################## Data cleaning ##############################################\n",
    "\n",
    "# Loop through each input dataframe and filter out rows where does not meet the clean up requirement\n",
    "# 1. data_id  is not present in qrels_data_ids\n",
    "# 2. relevant_score equal to 0\n",
    "# 3. duplicate ranking\n",
    "\n",
    "# Extract the valid values from the third column of qrels_df\n",
    "qrels_data_ids = set(qrels_df.iloc[:, 2])\n",
    "\n",
    "cleaned_dfs = []\n",
    "\n",
    "for df in input_dfs:\n",
    "    # Get rows where the data id is present in the qrels data ids\n",
    "    df_with_present_data_id = df.iloc[:, 2].isin(qrels_data_ids)\n",
    "    \n",
    "    # Get rows where relevant_score does not equal 0\n",
    "    df_with_relevant_score_not_zero = df.iloc[:, 4] != 0\n",
    "    \n",
    "    # Combine targeted rows for first and second rule\n",
    "    cleaned_df = df[df_with_present_data_id & df_with_relevant_score_not_zero]\n",
    "\n",
    "    # Get rows where ranking is not duplicated\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=[4])\n",
    "    cleaned_dfs.append(cleaned_df)\n",
    "\n",
    "# Concatenate all cleaned dataframes into a single dataframe (remove)\n",
    "cleaned_data = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "# add column names to the cleaned dataframe\n",
    "cleaned_data.columns = ['query_id', 'ignore', 'doc_id', 'rank', 'score', 'ignore2'] #label all the tables \n",
    "\n",
    "# Print the shape of the cleaned data\n",
    "print('Shape of cleaned_data:', cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['query_id', 'ignore', 'doc_id', 'rank', 'score', 'ignore2'], dtype='object')\n",
      "(290603, 6)\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data.columns)\n",
    "cleaned_data.head()\n",
    "print(cleaned_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>ignore_x</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>ignore2</th>\n",
       "      <th>ignore_y</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>401</td>\n",
       "      <td>Q0</td>\n",
       "      <td>FT924-5091</td>\n",
       "      <td>0</td>\n",
       "      <td>10015.175781</td>\n",
       "      <td>acsys8aln2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>Q0</td>\n",
       "      <td>FBIS3-39240</td>\n",
       "      <td>1</td>\n",
       "      <td>10014.111328</td>\n",
       "      <td>acsys8aln2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>401</td>\n",
       "      <td>Q0</td>\n",
       "      <td>FT924-4470</td>\n",
       "      <td>2</td>\n",
       "      <td>10013.638672</td>\n",
       "      <td>acsys8aln2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>401</td>\n",
       "      <td>Q0</td>\n",
       "      <td>FBIS4-18182</td>\n",
       "      <td>3</td>\n",
       "      <td>10013.591797</td>\n",
       "      <td>acsys8aln2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>Q0</td>\n",
       "      <td>FBIS3-59055</td>\n",
       "      <td>4</td>\n",
       "      <td>10013.441406</td>\n",
       "      <td>acsys8aln2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id ignore_x       doc_id  rank         score     ignore2  ignore_y  \\\n",
       "0       401       Q0   FT924-5091     0  10015.175781  acsys8aln2       0.0   \n",
       "1       401       Q0  FBIS3-39240     1  10014.111328  acsys8aln2       0.0   \n",
       "2       401       Q0   FT924-4470     2  10013.638672  acsys8aln2       0.0   \n",
       "3       401       Q0  FBIS4-18182     3  10013.591797  acsys8aln2       0.0   \n",
       "4       401       Q0  FBIS3-59055     4  10013.441406  acsys8aln2       0.0   \n",
       "\n",
       "   relevance  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        0.0  \n",
       "4        1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dictionary to store the system scores\n",
    "system_scores = {}\n",
    "\n",
    "# Merge the input file with the qrels to get the relevance scores\n",
    "merged_df = pd.merge(cleaned_data, qrels_df, on=['query_id', 'doc_id'], how='left')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision@10 and MAP per query\n",
    "grouped = merged_df.groupby('query_id')\n",
    "precisions = grouped.apply(lambda x: np.sum(x['relevance'].iloc[:10]) / 10)\n",
    "avg_precisions = grouped.apply(lambda x: np.sum(x['relevance'] * np.cumsum(x['relevance']) / np.arange(1, len(x['relevance']) + 1)) / np.sum(x['relevance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_id', 'ignore_x', 'doc_id', 'rank', 'score', 'ignore2',\n",
       "       'ignore_y', 'relevance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision@10</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.209015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.121394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.113520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.204275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.066068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     precision@10       MAP\n",
       "401           0.7  0.209015\n",
       "402           1.0  0.121394\n",
       "403           1.0  0.113520\n",
       "404           0.4  0.204275\n",
       "405           0.2  0.066068"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame with the query_id as the index and the precision@10 and MAP as columns\n",
    "results_df = pd.DataFrame({'precision@10': precisions, 'MAP': avg_precisions}, index=merged_df['query_id'].unique())\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('results.csv')\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall precision@10: 0.4720\n",
      "Overall MAP: 0.1371\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean precision@10 and mean MAP across all queries\n",
    "mean_precision = results_df['precision@10'].mean()\n",
    "mean_map = results_df['MAP'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overall precision@10: {mean_precision:.4f}\")\n",
    "print(f\"Overall MAP: {mean_map:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
